{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJn6bCxRPBl0"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WxYxEcvbmO1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.utils import get_file\n",
        "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
        "from imblearn.metrics import sensitivity_score, specificity_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEdulyfbcOif"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(7)\n",
        "np.random.seed(7)\n",
        "random.seed(7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjIFVP5LcR5p"
      },
      "outputs": [],
      "source": [
        "class_names = [\"benign\", \"malignant\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrLV6Jw4cVbZ"
      },
      "outputs": [],
      "source": [
        "def generate_csv(folder, label2int):\n",
        "    folder_name = os.path.basename(folder)\n",
        "    labels = list(label2int)\n",
        "    df = pd.DataFrame(columns=[\"filepath\", \"label\"])\n",
        "    i = 0\n",
        "    for label in labels:\n",
        "        print(\"Reading\", os.path.join(folder, label, \"*\"))\n",
        "        for filepath in glob.glob(os.path.join(folder, label, \"*\")):\n",
        "            df.loc[i] = [filepath, label2int[label]]\n",
        "            i += 1\n",
        "    output_file = f\"{folder_name}.csv\"\n",
        "    print(\"Saving\", output_file)\n",
        "    df.to_csv(output_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0o-TYfUccYG-"
      },
      "outputs": [],
      "source": [
        "generate_csv(\"/content/drive/MyDrive/SK_Pred/data/train\", {\"benign\": 0, \"melanoma\": 1})\n",
        "generate_csv(\"/content/drive/MyDrive/SK_Pred/data/valid\", {\"benign\": 0, \"melanoma\": 1})\n",
        "generate_csv(\"/content/drive/MyDrive/SK_Pred/data/test\", {\"benign\": 0, \"melanoma\": 1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ykROlJyc8EG"
      },
      "outputs": [],
      "source": [
        "train_metadata_filename = \"train.csv\"\n",
        "valid_metadata_filename = \"valid.csv\"\n",
        "df_train = pd.read_csv(train_metadata_filename)\n",
        "df_valid = pd.read_csv(valid_metadata_filename)\n",
        "n_training_samples = len(df_train)\n",
        "n_validation_samples = len(df_valid)\n",
        "print(\"Number of training samples:\", n_training_samples)\n",
        "print(\"Number of validation samples:\", n_validation_samples)\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((df_train[\"filepath\"], df_train[\"label\"]))\n",
        "valid_ds = tf.data.Dataset.from_tensor_slices((df_valid[\"filepath\"], df_valid[\"label\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvXDIEDydOC7"
      },
      "outputs": [],
      "source": [
        "def decode_img(img):\n",
        "  img = tf.image.decode_jpeg(img, channels=3)\n",
        "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "  return tf.image.resize(img, [299, 299])\n",
        "\n",
        "\n",
        "def process_path(filepath, label):\n",
        "  img = tf.io.read_file(filepath)\n",
        "  img = decode_img(img)\n",
        "  return img, label\n",
        "\n",
        "\n",
        "valid_ds = valid_ds.map(process_path)\n",
        "train_ds = train_ds.map(process_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIryhJ8jdWt0"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "optimizer = \"rmsprop\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p969J4dDdexq"
      },
      "outputs": [],
      "source": [
        "def prepare_for_training(ds, cache=True, batch_size=64, shuffle_buffer_size=1000):\n",
        "  if cache:\n",
        "    if isinstance(cache, str):\n",
        "      ds = ds.cache(cache)\n",
        "    else:\n",
        "      ds = ds.cache()\n",
        "  ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
        "\n",
        "  ds = ds.repeat()\n",
        "  ds = ds.batch(batch_size)\n",
        "\n",
        "  ds = ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "  return ds\n",
        "\n",
        "\n",
        "valid_ds = prepare_for_training(valid_ds, batch_size=batch_size, cache=\"valid-cached-data\")\n",
        "train_ds = prepare_for_training(train_ds, batch_size=batch_size, cache=\"train-cached-data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZCFBLPmdp1T"
      },
      "outputs": [],
      "source": [
        "batch = next(iter(valid_ds))\n",
        "\n",
        "def show_batch(batch):\n",
        "  plt.figure(figsize=(12,12))\n",
        "  for n in range(25):\n",
        "      ax = plt.subplot(5,5,n+1)\n",
        "      plt.imshow(batch[0][n])\n",
        "      plt.title(class_names[batch[1][n].numpy()].title())\n",
        "      plt.axis('off')\n",
        "        \n",
        "show_batch(batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJmjAW7neBoO"
      },
      "outputs": [],
      "source": [
        "module_url = \"https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/4\"\n",
        "m = tf.keras.Sequential([\n",
        "    hub.KerasLayer(module_url, output_shape=[2048], trainable=False),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "m.build([None, 299, 299, 3])\n",
        "m.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "m.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irGOFDupeNph"
      },
      "outputs": [],
      "source": [
        "model_name = f\"benign-vs-malignant_{batch_size}_{optimizer}\"\n",
        "tensorboard = tf.keras.callbacks.TensorBoard(log_dir=os.path.join(\"logs\", model_name))\n",
        "\n",
        "modelcheckpoint = tf.keras.callbacks.ModelCheckpoint(model_name + \"_{val_loss:.3f}.h5\", save_best_only=True, verbose=1)\n",
        "\n",
        "history = m.fit(train_ds, validation_data=valid_ds, \n",
        "                steps_per_epoch=n_training_samples // batch_size, \n",
        "                validation_steps=n_validation_samples // batch_size, verbose=1, epochs=100,\n",
        "                callbacks=[tensorboard, modelcheckpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UulFL1NGqV9x"
      },
      "outputs": [],
      "source": [
        "test_metadata_filename = \"test.csv\"\n",
        "df_test = pd.read_csv(test_metadata_filename)\n",
        "n_testing_samples = len(df_test)\n",
        "print(\"Number of testing samples:\", n_testing_samples)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((df_test[\"filepath\"], df_test[\"label\"]))\n",
        "\n",
        "def prepare_for_testing(ds, cache=True, shuffle_buffer_size=1000):\n",
        "\n",
        "  if cache:\n",
        "    if isinstance(cache, str):\n",
        "      ds = ds.cache(cache)\n",
        "    else:\n",
        "      ds = ds.cache()\n",
        "\n",
        "  ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
        "\n",
        "  return ds\n",
        "\n",
        "\n",
        "test_ds = test_ds.map(process_path)\n",
        "test_ds = prepare_for_testing(test_ds, cache=\"test-cached-data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJ9e1NoDqZIj"
      },
      "outputs": [],
      "source": [
        "y_test = np.zeros((n_testing_samples,))\n",
        "X_test = np.zeros((n_testing_samples, 299, 299, 3))\n",
        "for i, (img, label) in enumerate(test_ds.take(n_testing_samples)):\n",
        "  X_test[i] = img\n",
        "  y_test[i] = label.numpy()\n",
        "\n",
        "print(\"y_test.shape:\", y_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2n_XEE3qlIl"
      },
      "outputs": [],
      "source": [
        "\n",
        "m.load_weights(\"benign-vs-malignant_64_rmsprop_0.XXX.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZgifMv8r6YT"
      },
      "outputs": [],
      "source": [
        "print(\"Evaluating the model...\")\n",
        "loss, accuracy = m.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Loss:\", loss, \"  Accuracy:\", accuracy)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Nyla Pirani.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}